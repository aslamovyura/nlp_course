{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d471a22d",
   "metadata": {},
   "source": [
    "# Demo App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd50695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q kaggle\n",
    "\n",
    "# kaggle datasets download -d rajnathpatel/ner-data\n",
    "# # %mkdir ~/.kaggle\n",
    "# # !cp kaggle.json ~/.kaggle/\n",
    "# # !chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# !kaggle datasets download -d 'rajnathpatel/ner-data'\n",
    "# !unzip /content/ner-data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a38984",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0242f184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (4.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: filelock in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: seaborn in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from seaborn) (3.7.0)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.25 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.39.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "%pip install transformers\n",
    "%pip install seaborn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import  DistilBertForTokenClassification\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d85558d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b80a983",
   "metadata": {},
   "source": [
    "### Split Dataset\n",
    "\n",
    "Let’s load the first N rows of our dataframe and change the column names. Then split the dataframe into train, dev and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36861c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "N = 1_000\n",
    "df = pd.read_csv(\"ner.csv\").sample(frac=1)[:N]\n",
    "\n",
    "#change columns names\n",
    "df.rename(columns = {'text':'sentence', 'labels':'tags'}, inplace = True)\n",
    "\n",
    "#split train, dev , test sets\n",
    "df_train, df_dev, df_test = np.split(df.sample(frac=1, random_state=42),\n",
    "                            [int(.8 * len(df)), int(.9 * len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1bec3434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45438</th>\n",
       "      <td>A plane carrying Ethiopian-born Hamdi Issac , ...</td>\n",
       "      <td>O O O O B-per I-per O O O O B-per I-per O O B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10931</th>\n",
       "      <td>In recent years , Rwandan troops have entered ...</td>\n",
       "      <td>O B-tim O O B-gpe O O O B-geo O O O O O B-org ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22195</th>\n",
       "      <td>Two other Iranian-Americans - urban planner Ki...</td>\n",
       "      <td>O O O O O O B-per I-per O B-per I-per O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7861</th>\n",
       "      <td>The Washington Post quotes unidentified U.S. o...</td>\n",
       "      <td>B-org I-org I-org O O B-geo O O O O B-tim O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4492</th>\n",
       "      <td>He accused the network of possibly seeking to ...</td>\n",
       "      <td>O O O O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2980</th>\n",
       "      <td>The lengthy message criticizes the Saudi monar...</td>\n",
       "      <td>O O O O O B-geo O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5188</th>\n",
       "      <td>It says Burma 's human rights record makes it ...</td>\n",
       "      <td>O O B-geo O O O O O O O O O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25553</th>\n",
       "      <td>A KANGAROO hopping awkwardly along with some b...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21344</th>\n",
       "      <td>A total of 46 international referees will atte...</td>\n",
       "      <td>O O O O O O O O O O O O B-geo B-tim I-tim I-ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45629</th>\n",
       "      <td>He will be brought before a judge on Wednesday .</td>\n",
       "      <td>O O O O O O O O B-tim O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "45438  A plane carrying Ethiopian-born Hamdi Issac , ...   \n",
       "10931  In recent years , Rwandan troops have entered ...   \n",
       "22195  Two other Iranian-Americans - urban planner Ki...   \n",
       "7861   The Washington Post quotes unidentified U.S. o...   \n",
       "4492   He accused the network of possibly seeking to ...   \n",
       "...                                                  ...   \n",
       "2980   The lengthy message criticizes the Saudi monar...   \n",
       "5188   It says Burma 's human rights record makes it ...   \n",
       "25553  A KANGAROO hopping awkwardly along with some b...   \n",
       "21344  A total of 46 international referees will atte...   \n",
       "45629   He will be brought before a judge on Wednesday .   \n",
       "\n",
       "                                                    tags  \n",
       "45438  O O O O B-per I-per O O O O B-per I-per O O B-...  \n",
       "10931  O B-tim O O B-gpe O O O B-geo O O O O O B-org ...  \n",
       "22195  O O O O O O B-per I-per O B-per I-per O O O O ...  \n",
       "7861   B-org I-org I-org O O B-geo O O O O B-tim O O ...  \n",
       "4492                       O O O O O O O O O O O O O O O  \n",
       "...                                                  ...  \n",
       "2980   O O O O O B-geo O O O O O O O O O O O O O O O ...  \n",
       "5188                   O O B-geo O O O O O O O O O O O O  \n",
       "25553  O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
       "21344  O O O O O O O O O O O O B-geo B-tim I-tim I-ti...  \n",
       "45629                            O O O O O O O O B-tim O  \n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7f3ef21",
   "metadata": {},
   "source": [
    "### DistilbertNer Class\n",
    "\n",
    "The init method takes into input the dimensions of the classification, so the number of tokens that we can predict, and instantiate the pretrained model.\n",
    "\n",
    "The forward computation simply takes the inputs_ids (the tokens) and the attention_mask (array of 0/1 that tells us if the token is a PAD or not) and returns in output a dictionary : {loss, logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b730001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilbertNER(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement NN class based on distilbert pretrained from Hugging face.\n",
    "    Inputs : \n",
    "    tokens_dim : int specifyng the dimension of the classifier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens_dim):\n",
    "        super(DistilbertNER,self).__init__()\n",
    "\n",
    "        if type(tokens_dim) != int:\n",
    "                raise TypeError('Please tokens_dim should be an integer')\n",
    "\n",
    "        if tokens_dim <= 0:\n",
    "                raise ValueError('Classification layer dimension should be at least 1')\n",
    "\n",
    "        #set the output of each token classifier = unique_lables\n",
    "        self.pretrained = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels = tokens_dim) \n",
    "\n",
    "    #labels are needed in order to compute the loss\n",
    "    def forward(self, input_ids, attention_mask, labels = None): \n",
    "        \"\"\"\n",
    "        Forwad computation of the network\n",
    "        Input:\n",
    "        - inputs_ids : from model tokenizer\n",
    "        - attention :  mask from model tokenizer\n",
    "        - labels : if given the model is able to return the loss value\n",
    "        \"\"\"\n",
    "\n",
    "        #inference time no labels\n",
    "        if labels == None:\n",
    "            out = self.pretrained(input_ids = input_ids, attention_mask = attention_mask )\n",
    "            return out\n",
    "\n",
    "        out = self.pretrained(input_ids = input_ids, attention_mask = attention_mask , labels = labels)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5487bf92",
   "metadata": {},
   "source": [
    "### NerDataset Class\n",
    "\n",
    "The second class it’s an extension of the module nn.Dataset, which enables us to create our custom dataset.\n",
    "\n",
    "Given in input a dataframe this method will tokenize the text and match the additional generated tokens with the correct tag (as we described in the previous article). Now you can index the dataset which will return a batch of texts and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc2bb3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset implementation to get (text,labels) tuples\n",
    "    Inputs:\n",
    "    - df : dataframe with columns [tags, sentence]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError('Input should be a dataframe')\n",
    "\n",
    "        if \"tags\" not in df.columns or \"sentence\" not in df.columns:\n",
    "            raise ValueError(\"Dataframe should contain 'tags' and 'sentence' columns\")\n",
    "\n",
    "        \n",
    "\n",
    "        tags_list = [i.split() for i in df[\"tags\"].values.tolist()]\n",
    "        texts = df[\"sentence\"].values.tolist()\n",
    "\n",
    "        self.texts = [tokenizer(text, padding = \"max_length\", truncation = True, return_tensors = \"pt\") for text in texts]\n",
    "        self.labels = [match_tokens_labels(text, tags) for text,tags in zip(self.texts, tags_list)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_text = self.texts[idx]\n",
    "        batch_labels = self.labels[idx]\n",
    "\n",
    "        return batch_text, torch.LongTensor(batch_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6face6fa",
   "metadata": {},
   "source": [
    "### MetricsTracking Class\n",
    "\n",
    "Sometimes it’s annoying to compute all the metrics inside the train loop, that’s why this class helps us do that. You only need to instantiate a new object and call the update method every time you make predictions over a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "355e6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsTracking():\n",
    "    \"\"\"\n",
    "    In order make the train loop lighter I define this class to track all the metrics that we are going to measure for our model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "\n",
    "        self.total_acc = 0\n",
    "        self.total_f1 = 0\n",
    "        self.total_precision = 0\n",
    "        self.total_recall = 0\n",
    "\n",
    "    def update(self, predictions, labels , ignore_token = -100):\n",
    "        '''\n",
    "        Call this function every time you need to update your metrics.\n",
    "        Where in the train there was a -100, were additional token that we dont want to label, so remove them.\n",
    "        If we flatten the batch its easier to access the indexed = -100\n",
    "        '''  \n",
    "        predictions = predictions.flatten()\n",
    "        labels = labels.flatten()\n",
    "\n",
    "        predictions = predictions[labels != ignore_token]\n",
    "        labels = labels[labels != ignore_token]\n",
    "\n",
    "        predictions = predictions.to(\"cpu\")\n",
    "        labels = labels.to(\"cpu\")\n",
    "\n",
    "        acc = accuracy_score(labels,predictions)\n",
    "        f1 = f1_score(labels, predictions, average = \"macro\")\n",
    "        precision = precision_score(labels, predictions, average = \"macro\")\n",
    "        recall = recall_score(labels, predictions, average = \"macro\")\n",
    "\n",
    "        self.total_acc  += acc\n",
    "        self.total_f1 += f1\n",
    "        self.total_precision += precision\n",
    "        self.total_recall  += recall\n",
    "\n",
    "    def return_avg_metrics(self,data_loader_size):\n",
    "        n = data_loader_size\n",
    "        metrics = {\n",
    "            \"acc\": round(self.total_acc / n ,3), \n",
    "            \"f1\": round(self.total_f1 / n, 3), \n",
    "            \"precision\" : round(self.total_precision / n, 3), \n",
    "            \"recall\": round(self.total_recall / n, 3)\n",
    "                }\n",
    "        return metrics "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06b11b9a",
   "metadata": {},
   "source": [
    "### Custom Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9bf82bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_2_labels(tags : str, tag2idx : dict):\n",
    "    '''\n",
    "    Method that takes a list of tags and a dictionary mapping and returns a list of labels (associated).\n",
    "    Used to create the \"label\" column in df from the \"tags\" column.\n",
    "    '''\n",
    "    return [tag2idx[tag] if tag in tag2idx else unseen_label for tag in tags.split()] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cbbe354",
   "metadata": {},
   "source": [
    "__tags_mapping__ : takes in input the tags column of a dataframe and returns : (1) a dictionary mapping tags to indexes (label) (2) dictionary mapping indexes to tags (3) the label corresponding to tag O (4) a set of unique tags encountered in the train data which will define the classifier dimension."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35c81e74",
   "metadata": {},
   "source": [
    "__tags_2_labels__ : Method that takes a list of tags and a dictionary mapping tags to labels, and returns a list of labels associated to the original tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc69d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_mapping(tags_series : pd.Series):\n",
    "    \"\"\"\n",
    "    tag_series = df column with tags for each sentence.\n",
    "    Returns:\n",
    "    - dictionary mapping tags to indexes (label)\n",
    "    - dictionary mappign inedexes to tags\n",
    "    - The label corresponding to tag 'O'\n",
    "    - A set of unique tags ecountered in the trainind df, this will define the classifier dimension\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(tags_series, pd.Series):\n",
    "        raise TypeError('Input should be a padas Series')\n",
    "\n",
    "    unique_tags = set()\n",
    "\n",
    "    for tag_list in df_train[\"tags\"]:\n",
    "        for tag in tag_list.split():\n",
    "            unique_tags.add(tag)\n",
    "\n",
    "    tag2idx = {k:v for v,k in enumerate(sorted(unique_tags))}\n",
    "    idx2tag = {k:v for v,k in tag2idx.items()}\n",
    "\n",
    "    unseen_label = tag2idx[\"O\"]\n",
    "\n",
    "    return tag2idx, idx2tag, unseen_label, unique_tags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5fc9f96",
   "metadata": {},
   "source": [
    "__match_tokens_labels__ : from the tokenized text and the original tags (which are associated with words not token) it outputs an array of tags for every single token. It associates a token with the tag of its original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43fbb4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_tokens_labels(tokenized_input, tags, ignore_token = -100):\n",
    "    '''\n",
    "    Used in the custom dataset.\n",
    "    -100 will be tha label used to match additional tokens like [CLS] [PAD] that we dont care about. \n",
    "    Inputs : \n",
    "        - tokenized_input : tokenizer over the imput text -> {input_ids, attention_mask}\n",
    "        - tags : is a single label array -> [O O O O O O O O O O O O O O B-tim O]\n",
    "    \n",
    "    Returns a list of labels that match the tokenized text -> [-100, 3,5,6,-100,...]\n",
    "    '''\n",
    "\n",
    "    #gives an array [ None , 0 , 1 ,2 ,... None]. Each index tells the word of reference of the token\n",
    "    word_ids = tokenized_input.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(ignore_token)\n",
    "\n",
    "        #if its equal to the previous word we can add the same label id of the provious or -100 \n",
    "        else :\n",
    "            try:\n",
    "                reference_tag = tags[word_idx]\n",
    "                label_ids.append(tag2idx[reference_tag])\n",
    "            except:\n",
    "                label_ids.append(ignore_token)\n",
    "            \n",
    "        \n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acc35813",
   "metadata": {},
   "source": [
    "__freeze_model__ : freezes last num_layers of a model to prevent catastrophic forgetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5044201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model,num_layers = 1):\n",
    "    \"\"\"\n",
    "    Freeze last num_layers of a model to prevent ctastrophic forgetting.\n",
    "    Doesn't seem to work weel, its better to fine tune the entire netwok\n",
    "    \"\"\"\n",
    "    for id , params in enumerate(model.parameters()):\n",
    "        if id == len(list(model.parameters())) - num_layers: \n",
    "            print(\"last layer unfreezed\")\n",
    "            params.requires_grad = True\n",
    "        else:\n",
    "            params.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44fb5910",
   "metadata": {},
   "source": [
    "__train_loop__ : usual train loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd74699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_dataset, dev_dataset, optimizer,  batch_size, epochs):\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(epochs) : \n",
    "\n",
    "        train_metrics = MetricsTracking()\n",
    "        total_loss_train = 0\n",
    "\n",
    "        model.train() #train mode\n",
    "\n",
    "        for train_data, train_label in tqdm(train_dataloader):\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            '''\n",
    "            squeeze in order to match the sizes. From [batch,1,seq_len] --> [batch,seq_len] \n",
    "            '''\n",
    "            mask = train_data['attention_mask'].squeeze(1).to(device)\n",
    "            input_id = train_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_id, mask, train_label)\n",
    "            loss, logits = output.loss, output.logits\n",
    "            predictions = logits.argmax(dim= -1) \n",
    "\n",
    "            #compute metrics\n",
    "            train_metrics.update(predictions, train_label)\n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "            #grad step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        '''\n",
    "        EVALUATION MODE\n",
    "        '''            \n",
    "        model.eval()\n",
    "\n",
    "        dev_metrics = MetricsTracking()\n",
    "        total_loss_dev = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for dev_data, dev_label in dev_dataloader:\n",
    "\n",
    "                dev_label = dev_label.to(device)\n",
    "\n",
    "                mask = dev_data['attention_mask'].squeeze(1).to(device)\n",
    "                input_id = dev_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask, dev_label)\n",
    "                loss, logits = output.loss, output.logits\n",
    "\n",
    "                predictions = logits.argmax(dim= -1)     \n",
    "\n",
    "                dev_metrics.update(predictions, dev_label)\n",
    "                total_loss_dev += loss.item()\n",
    "\n",
    "        train_results = train_metrics.return_avg_metrics(len(train_dataloader))\n",
    "        dev_results = dev_metrics.return_avg_metrics(len(dev_dataloader))\n",
    "\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            clear_output(True)\n",
    "            \n",
    "        print(f\"EPOCH: #{epoch}\")\n",
    "        print(f\"TRAIN \\nLoss: {total_loss_train / len(train_dataset)} \\nMetrics {train_results}\\n\" ) \n",
    "        print(f\"VALIDATION \\nLoss {total_loss_dev / len(dev_dataset)} \\nMetrics{dev_results}\\n\" ) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "810f2534",
   "metadata": {},
   "source": [
    "### Main\n",
    "Now let’s use everything in the main scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f415a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tag-label mapping\n",
    "tag2idx, idx2tag , unseen_label, unique_tags = tags_mapping(df_train[\"tags\"])\n",
    "\n",
    "#create the label column from tag. Unseen labels will be tagged as \"O\"\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"labels\"] = df[\"tags\"].apply(lambda tags : tags_2_labels(tags, tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8cf045ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-art': 0, 'B-eve': 1, 'B-geo': 2, 'B-gpe': 3, 'B-nat': 4, 'B-org': 5, 'B-per': 6, 'B-tim': 7, 'I-art': 8, 'I-eve': 9, 'I-geo': 10, 'I-gpe': 11, 'I-nat': 12, 'I-org': 13, 'I-per': 14, 'I-tim': 15, 'O': 16}\n"
     ]
    }
   ],
   "source": [
    "print(tag2idx)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8a946b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original text\n",
    "text = df_train[\"sentence\"].values.tolist()\n",
    "\n",
    "#toeknized text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "text_tokenized = tokenizer(text , padding = \"max_length\" , truncation = True, return_tensors = \"pt\" )\n",
    "\n",
    "#mapping token to original word\n",
    "word_ids = text_tokenized.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6de763e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World oil prices hit new record highs Wednesday , as experts worried about production problems that could disrupt output .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2088,  3514,  ...,     0,     0,     0],\n",
       "        [  101,  2343, 11531,  ...,     0,     0,     0],\n",
       "        [  101,  2012,  2560,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  9390,  2360,  ...,     0,     0,     0],\n",
       "        [  101,  5606,  1997,  ...,     0,     0,     0],\n",
       "        [  101,  1037, 14056,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(text[0])\n",
    "text_tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d4107e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Loss: 0.00163236350286752 \n",
      "Metrics {'acc': 0.984, 'f1': 0.906, 'precision': 0.92, 'recall': 0.9}\n",
      "\n",
      "VALIDATION \n",
      "Loss 0.01762304276227951 \n",
      "Metrics{'acc': 0.923, 'f1': 0.639, 'precision': 0.673, 'recall': 0.625}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:08<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Loss: 0.001338017259258777 \n",
      "Metrics {'acc': 0.987, 'f1': 0.914, 'precision': 0.926, 'recall': 0.909}\n",
      "\n",
      "VALIDATION \n",
      "Loss 0.014924895949661732 \n",
      "Metrics{'acc': 0.937, 'f1': 0.683, 'precision': 0.74, 'recall': 0.662}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DistilbertNER(len(unique_tags))\n",
    "\n",
    "\n",
    "#Prevent Catastrofic Forgetting\n",
    "# model = freeze_model(model, num_layers = 2)\n",
    "\n",
    "#datasets\n",
    "train_dataset = NerDataset(df_train)\n",
    "dev_dataset = NerDataset(df_dev)\n",
    "\n",
    "lr = 1e-2\n",
    "optimizer = SGD(model.parameters(), lr=lr, momentum = 0.9)  \n",
    "\n",
    "\n",
    "#MAIN\n",
    "parameters = {\n",
    "    \"model\": model,\n",
    "    \"train_dataset\": train_dataset,\n",
    "    \"dev_dataset\" : dev_dataset,\n",
    "    \"optimizer\" : optimizer,\n",
    "    \"batch_size\" : 32,\n",
    "    \"epochs\" : 25\n",
    "}\n",
    "\n",
    "train_loop(**parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07c174a7",
   "metadata": {},
   "source": [
    "### Prediction unitilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "969bc615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "import typing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "edd073a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_render(tokens: tp.Sequence[str], ner_tags: tp.Sequence[int], title: tp.Optional[str] = None, **kwargs):\n",
    "    pos = 0\n",
    "    ents = []\n",
    "    for word, tag_ in zip(tokens, ner_tags):\n",
    "        # print(tag_)\n",
    "        tag = idx2tag[tag_]\n",
    "        if tag.startswith('B'):\n",
    "            ents.append({\n",
    "                \"start\": pos,\n",
    "                \"end\": pos + len(word),\n",
    "                \"label\": tag.split(\"-\")[1]\n",
    "            })\n",
    "        elif tag.startswith('I'):\n",
    "            ents[-1][\"end\"] = pos + len(word)\n",
    "        pos += (len(word) + 1)\n",
    "    displacy.render({\n",
    "        \"text\": \" \".join(tokens),\n",
    "        \"ents\": ents,\n",
    "        \"title\": title\n",
    "    }, style=\"ent\", manual=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "205f8c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence: str):\n",
    "\n",
    "    # encoded = tokenizer(sentence, truncation = True, return_tensors = \"pt\").to(device)\n",
    "    encoded = tokenizer(sentence, return_tensors = \"pt\").to(device)\n",
    "    output = model(encoded[\"input_ids\"], attention_mask=encoded[\"attention_mask\"])\n",
    "\n",
    "    logits = output.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"][0])\n",
    "    tokens = tokens[1:-1]\n",
    "\n",
    "    tags = predictions[0].cpu().numpy()\n",
    "    tags = tags[1:-1]\n",
    "\n",
    "    labels = [idx2tag[t] for t in tags]\n",
    "    print(labels)\n",
    "\n",
    "    ner_render(tokens, tags)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2839613e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-per', 'I-per', 'O', 'O', 'O', 'B-tim', 'I-tim', 'I-tim', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    mr smith\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">per</span>\n",
       "</mark>\n",
       " was born on \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    10 december ,\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">tim</span>\n",
       "</mark>\n",
       " or two weeks before christmas .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(\"Mr Smith was born on 10 December, or two weeks before Christmas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "38c108ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'B-per', 'B-per', 'O']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">my name is \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    yu\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">per</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##ry\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">per</span>\n",
       "</mark>\n",
       " !</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(\"My name is Yury!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "210e9e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'B-geo', 'O']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">i am from \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    minsk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">geo</span>\n",
       "</mark>\n",
       " !</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(\"I am from Minsk!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "de2e1787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16 16 16 16 16  7  7 16]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">i will be there at \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">tim</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    pm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">tim</span>\n",
       "</mark>\n",
       " !</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(\"I will be there at 3 pm!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "06bded97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-geo', 'O', 'O', 'O', 'O', 'B-geo', 'O']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    germany\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">geo</span>\n",
       "</mark>\n",
       " is the heart of \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    europe\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">geo</span>\n",
       "</mark>\n",
       " .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(\"Germany is the heart of Europe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5b1e2e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-org', 'B-org', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ca\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">org</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##e\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">org</span>\n",
       "</mark>\n",
       " production is the best .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(\"CAE production is the best.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "059e49dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-org', 'B-org', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ama\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">org</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ##t\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">org</span>\n",
       "</mark>\n",
       " / applied materials etc ##her is used for semiconductor industry .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(\"AMAT / APPLIED MATERIALS etcher is used for semiconductor industry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9f056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
