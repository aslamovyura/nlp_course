{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.get_device_capability() < (7, 5):\n",
    "    raise ValueError(f\"You got a GPU with capability {torch.cuda.get_device_capability()}, need at least (7, 5)\")\n",
    "else: print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes==0.37.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (0.37.0)\n",
      "Requirement already satisfied: transformers==4.27.4 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (4.27.4)\n",
      "Requirement already satisfied: datasets==2.7.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: accelerate==0.18.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (0.18.0)\n",
      "Requirement already satisfied: loralib==0.1.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: peft==0.3.0.dev0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (0.3.0.dev0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers==4.27.4) (2022.10.31)\n",
      "Requirement already satisfied: requests in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers==4.27.4) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers==4.27.4) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers==4.27.4) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers==4.27.4) (0.13.2)\n",
      "Requirement already satisfied: filelock in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers==4.27.4) (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers==4.27.4) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers==4.27.4) (23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from transformers==4.27.4) (0.13.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from datasets==2.7.0) (2023.3.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from datasets==2.7.0) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from datasets==2.7.0) (3.8.4)\n",
      "Requirement already satisfied: pandas in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from datasets==2.7.0) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from datasets==2.7.0) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from datasets==2.7.0) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from datasets==2.7.0) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from datasets==2.7.0) (0.70.14)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from accelerate==0.18.0) (2.0.0)\n",
      "Requirement already satisfied: psutil in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from accelerate==0.18.0) (5.9.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from aiohttp->datasets==2.7.0) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from aiohttp->datasets==2.7.0) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from aiohttp->datasets==2.7.0) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from aiohttp->datasets==2.7.0) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from aiohttp->datasets==2.7.0) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from aiohttp->datasets==2.7.0) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from aiohttp->datasets==2.7.0) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.4) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from requests->transformers==4.27.4) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from requests->transformers==4.27.4) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from requests->transformers==4.27.4) (3.4)\n",
      "Requirement already satisfied: jinja2 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (11.7.101)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (2.0.0)\n",
      "Requirement already satisfied: sympy in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (3.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from torch>=1.4.0->accelerate==0.18.0) (11.7.99)\n",
      "Requirement already satisfied: wheel in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate==0.18.0) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate==0.18.0) (65.6.3)\n",
      "Requirement already satisfied: cmake in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate==0.18.0) (3.26.0)\n",
      "Requirement already satisfied: lit in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate==0.18.0) (15.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from pandas->datasets==2.7.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from pandas->datasets==2.7.0) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.7.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->accelerate==0.18.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages (from sympy->torch>=1.4.0->accelerate==0.18.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bitsandbytes==0.37.0 transformers==4.27.4 datasets==2.7.0 accelerate==0.18.0 loralib==0.1.1 peft==0.3.0.dev0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 46/46 [00:25<00:00,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "CACHE_DIR = '/media/tfsservices/DATA/NLP/cache/'\n",
    "# MODEL_NAME = \"facebook/opt-6.7b\"\n",
    "# MODEL_NAME, ADAPTER_PATH = \"facebook/opt-30b\", \"adapters/lora-adapters-30b\"\n",
    "# MODEL_NAME, ADAPTER_PATH = \"EleutherAI/gpt-j-6B\", \"adapters/gpt-j-6B-marketing\"\n",
    "# MODEL_NAME, ADAPTER_PATH = \"EleutherAI/gpt-j-6B\", \"adapters/gpt-j-6B-wm\"\n",
    "\n",
    "MODEL_NAME = \"EleutherAI/gpt-neox-20B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    load_in_8bit=True,        # bitsandbytes lib required (convert the loaded model into mixed-8bit quantized model.)\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=CACHE_DIR,      # path to a directory in which a downloaded pretrained model\n",
    "    # low_cpu_mem_usage=True,   # loads the model using ~1x model size CPU memory\n",
    "    # offload_state_dict=True)  # temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU RAM\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "        self.ENCOUNTERS = encounters\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        stop_count = 0\n",
    "        for stop in self.stops:\n",
    "            stop_count = (stop == input_ids[0]).sum().item()\n",
    "\n",
    "        if stop_count >= self.ENCOUNTERS:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ids = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in [\"###\"]]\n",
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids, encounters=1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_generate_marketing():\n",
    "\n",
    "#     inputs = tokenizer(\"Hi {FirstName}\", return_tensors=\"pt\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_length=200)\n",
    "#         print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, max_length=250):\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_length=max_length, do_sample=True, )\n",
    "        outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"),\n",
    "                                # early_stopping=True, \n",
    "                                # num_beams=5,\n",
    "                                temperature=0.7,\n",
    "                                top_p=0.8,\n",
    "                                do_sample=True,\n",
    "                                max_length = max_length,\n",
    "                                # pad_token_id=tokenizer.eos_token_id,\n",
    "                                stopping_criteria=stopping_criteria\n",
    "                                # output_scores=True,\n",
    "                                # return_dict_in_generate=True).detach()\n",
    "        )\n",
    "        print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate text using raw LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:220: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:493.)\n",
      "  attn_scores = torch.where(causal_mask, attn_scores, mask_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi {FirstName} \n",
      "{LastName}\n",
      "\n",
      "{Address}\n",
      "{City}\n",
      "{State}\n",
      "{Zip}\n",
      "\n",
      "{Phone}\n",
      "{Email}\n",
      "\n",
      "{URL}\n",
      "\n",
      "{Hobbies}\n",
      "{Website}\n",
      "\n",
      "{Birthday}\n",
      "{Gender}\n",
      "{Height}\n",
      "{Weight}\n",
      "{DateOfBirth}\n",
      "{DateOfDeath}\n",
      "\n",
      "{Humor}\n",
      "{HumorLevel}\n",
      "{HumorSense}\n",
      "{HumorType}\n",
      "\n",
      "{FavoriteColor}\n",
      "{FavoriteColorHex}\n",
      "{FavoriteColorRGB}\n",
      "{FavoriteColorName}\n",
      "\n",
      "{FavoriteFood}\n",
      "{FavoriteFoodHex}\n",
      "{FavoriteFoodRGB}\n",
      "{FavoriteFoodName}\n",
      "\n",
      "{FavoriteDrink}\n",
      "{FavoriteDrinkHex}\n",
      "{FavoriteDrinkRGB}\n",
      "{FavoriteDrinkName}\n",
      "\n",
      "{FavoriteSport}\n",
      "{FavoriteSportHex}\n",
      "{FavoriteSportRGB}\n",
      "{FavoriteSportName}\n",
      "\n",
      "{FavoriteActivity}\n",
      "{FavoriteActivityHex}\n",
      "{FavoriteActivityRGB}\n",
      "{F\n"
     ]
    }
   ],
   "source": [
    "generate(\"Hi {FirstName} \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and apply adapter to generate responses for Web Messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "model_id = \"adapters/gpt-neox-20B-wm\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(model_id)\n",
    "model = PeftModel.from_pretrained(model, model_id, device_map={\"\":0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi {FirstName} \n",
      "I received your email. I am checking the photos right now. \n",
      "I will get back to you within 24 hours. \n",
      "{SenderSignature} \n",
      " ###\n"
     ]
    }
   ],
   "source": [
    "generate(\"Hi {FirstName} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi {FirstName} \n",
      "Thank you for reaching out to us. \n",
      "Do you have the unit you are looking to sell? \n",
      "I am {Sender} from CAE. I will be looking at the unit you are listing. \n",
      "Please let me know if you have any questions. \n",
      "Best regards, \n",
      "{SendersSignature} \n",
      " ###\n"
     ]
    }
   ],
   "source": [
    "generate(\"Hi {FirstName} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi {FirstName} \n",
      "I received your inquiry for {MakeModel} \n",
      "Do you have a photo of the machine? \n",
      "Also, can you send me a list of what you have available and the asking price? \n",
      "I will review and let you know if I have any questions. \n",
      "Thanks \n",
      "{SenderSignature} \n",
      " ###\n"
     ]
    }
   ],
   "source": [
    "generate(\"Hi {FirstName} \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Adapter. Apply adapter to generate text for ARMM campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f622a734f20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"adapters/gpt-neox-20B-armm\"\n",
    "model = PeftModel.from_pretrained(model.base_model.model, model_id, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi {FirstName} \n",
      "\n",
      "\n",
      "\n",
      "We have a client that is looking for {MakeModel} for their project. They need the system for the next week, and they have a budget of {Price} for the system.\n",
      "\n",
      "They are willing to pay up front, so please let me know if you have one for sale.\n",
      "\n",
      "\n",
      "\n",
      "{Listing}\n",
      "\n",
      "\n",
      "\n",
      "Best regards,\n",
      "\n",
      "{SendersSignature} \n",
      " ###\n"
     ]
    }
   ],
   "source": [
    "generate(\"Hi {FirstName} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi {FirstName} \n",
      "\n",
      "\n",
      "\n",
      "I am working with a client who is looking for a {MakeModel} system to complete a project. \n",
      "\n",
      "The seller has decided to release the system to the market and they are now looking for a buyer who is willing to pay a reasonable price.\n",
      "\n",
      "The seller is asking {Price} for the system.\n",
      "\n",
      "They are also willing to consider a buyer who is willing to pay {Price} for the system. \n",
      "\n",
      "Please review the photos and let me know if you are interested. \n",
      "\n",
      "Also, if you have other equipment for sale, please let me know.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "{SendersSignature} \n",
      " ###\n"
     ]
    }
   ],
   "source": [
    "generate(\"Hi {FirstName} \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Adapter again. Apply adapter to generate respnses for Web Messages ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"adapters/gpt-neox-20B-wm\"\n",
    "# PeftModel.disable_adapter(model)\n",
    "model = PeftModel.from_pretrained(model.base_model.model, model_id, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi {FirstName} \n",
      "Thank you for reaching out.  I'm checking our availability for the {MakeModel} you are looking to sell.  Do you have photos and information about the unit?  \n",
      "{Listing} \n",
      "Best regards, \n",
      "{SenderSignature} \n",
      " ###\n"
     ]
    }
   ],
   "source": [
    "generate(\"Hi {FirstName} \")\n",
    "# generate(\"Hi inquiry \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi {FirstName} \n",
      "Hope you are doing well. \n",
      "I received your email. \n",
      "I am checking availability and price for you. \n",
      "Could you please send me the photos of the machine you have for sale? \n",
      "I will forward this to our client and they will get back to you. \n",
      "Thanks, \n",
      "{SendersSignature} \n",
      " ###\n"
     ]
    }
   ],
   "source": [
    "generate(\"Hi {FirstName} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Adapter again. Apply adapter to generate text for ARMM campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"adapters/gpt-neox-20B-armm\"\n",
    "PeftModel.disable_adapter(model)\n",
    "\n",
    "model = PeftModel.from_pretrained(model.base_model.model, model_id, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi {FirstName} \n",
      "\n",
      "\n",
      "\n",
      "Hope this mail finds you well.\n",
      "\n",
      "Our client is looking for {MakeModel} for their new project, and they are willing to pay up to {Price} for the complete unit. They are also looking for {MakeModel} as well, but they prefer to buy one complete unit instead of two.\n",
      "\n",
      "They are looking for the complete unit, so they can start their project immediately. If you have one available, please let me know ASAP. \n",
      "\n",
      "Also, if you have any other similar tools for sale, please let me know. I am looking for all kinds of tools.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "{SendersSignature} \n",
      " ###\n"
     ]
    }
   ],
   "source": [
    "generate(\"Hi {FirstName} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi {FirstName} \n",
      "\n",
      "\n",
      "\n",
      "We are currently looking for a {MakeModel} for our client.\n",
      "\n",
      "They are looking for a new {MakeModel} as they are looking to replace their current {MakeModel} due to the tool being out of production for the last few years.\n",
      "\n",
      "They are looking for a complete tool with all the necessary components, as they are going to use this tool for their production line.\n",
      "\n",
      "Please let me know if you have any available for sale or if you know of any coming available.\n",
      "\n",
      "We are also looking for other equipment, please let me know if you have any other equipment for sale.\n",
      "\n",
      "Thank you.\n",
      "\n",
      "\n",
      "\n",
      "{SendersSignature} \n",
      " ###\n"
     ]
    }
   ],
   "source": [
    "generate(\"Hi {FirstName} \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.disable_adapter_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type='LORA', base_model_name_or_path='EleutherAI/gpt-neox-20B', task_type='CAUSAL_LM', inference_mode=True, r=16, target_modules=['query_key_value'], lora_alpha=32, lora_dropout=0.1, merge_weights=False, fan_in_fan_out=True, enable_lora=[True, False, True], bias='none', modules_to_save=None, init_lora_weights=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi {FirstName} \n",
      "<br/>\n",
      "<br/>\n",
      "<a href='{SiteUrl}'>Go back to {FirstName}'s profile</a>\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following:\n",
      "{FirstName} {LastName}\n",
      "\n",
      "You can use the following to get the profile URL:\n",
      "http://{SiteUrl}/profile/{UserId}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate(\"Hi {FirstName} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi {FirstName}  {LastName}\",\n",
      "                        new { @class = \"btn btn-default\" })\n",
      "                </div>\n",
      "            </div>\n",
      "        }\n",
      "    </div>\n",
      "}\n",
      "\n",
      "@section scripts\n",
      "{\n",
      "    <script type=\"text/javascript\">\n",
      "        $(function () {\n",
      "            $('.datepicker').datepicker({\n",
      "                showOn: 'button',\n",
      "                buttonImage: '~/Content/images/calendar.png',\n",
      "                buttonImageOnly: true\n",
      "            });\n",
      "        });\n",
      "    </script>\n",
      "}\n",
      "\n",
      "The problem is that when I run the application, I get the following error:\n",
      "\n",
      "The name 'FirstName' does not exist in the current context\n",
      "\n",
      "I am using a model class named User, and the FirstName and LastName properties are in the User class. I have tried to make the User class public, but it did not help. I have also tried to make the User class internal, but it did not help either.\n",
      "How can I make the FirstName and LastName properties available to the view?\n",
      "\n",
      "A:\n",
      "\n",
      "I am not sure if you are using a view model or not, but if you\n"
     ]
    }
   ],
   "source": [
    "generate(\"Hi {FirstName}  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
