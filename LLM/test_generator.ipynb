{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.get_device_capability() < (7, 5):\n",
    "    raise ValueError(f\"You got a GPU with capability {torch.cuda.get_device_capability()}, need at least (7, 5)\")\n",
    "else: print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q bitsandbytes==0.37.0 transformers==4.27.4 datasets==2.7.0 accelerate==0.18.0 loralib==0.1.1 peft==0.3.0.dev0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tfsservices/miniconda3/envs/nlp_test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 46/46 [00:27<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "CACHE_DIR = '/media/tfsservices/DATA/NLP/cache/'\n",
    "MODEL_NAME = \"EleutherAI/gpt-neox-20B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    load_in_8bit=True,        # bitsandbytes lib required (convert the loaded model into mixed-8bit quantized model.)\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=CACHE_DIR,      # path to a directory in which a downloaded pretrained model\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, stops = [], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "        self.ENCOUNTERS = encounters\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        stop_count = 0\n",
    "        for stop in self.stops:\n",
    "            stop_count = (stop == input_ids[0]).sum().item()\n",
    "\n",
    "        if stop_count >= self.ENCOUNTERS:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ids1 = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in [\"###\"]]\n",
    "stopping_criteria1 = StoppingCriteriaSub(stops=stop_words_ids1, encounters=1)\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([stopping_criteria1])\n",
    "\n",
    "# stop_words_ids2 = [tokenizer(stop_word, return_tensors='pt')['input_ids'].squeeze() for stop_word in [\"A:\"]]\n",
    "# stopping_criteria2 = StoppingCriteriaSub(stops=stop_words_ids2, encounters=2)\n",
    "\n",
    "# stopping_criteria = StoppingCriteriaList([stopping_criteria1, stopping_criteria2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_promt(generated: str, prompt: str):\n",
    "    '''\n",
    "    Remove original promt and specific signs from the model output.\n",
    "    '''\n",
    "    prompt_length = len(prompt)\n",
    "    output = generated[prompt_length:]\n",
    "    output = output.replace(\"###\", \"\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prompt: str, max_length=250):\n",
    "\n",
    "    batch = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "    # with torch.cuda.amp.autocast():\n",
    "        \n",
    "        outputs = model.generate(**batch,\n",
    "                                # early_stopping=True, \n",
    "                                # num_beams=6,\n",
    "                                # num_beam_groups = 3,\n",
    "                                # no_repeat_ngram_size = 4,\n",
    "                                # temperature=0.9,\n",
    "                                top_k=10,\n",
    "                                # top_p=0.8,\n",
    "                                # penalty_alpha=0.6,\n",
    "                                # repetition_penalty = 1.05,\n",
    "                                do_sample=True,\n",
    "\n",
    "                                max_length = max_length,\n",
    "                                pad_token_id=tokenizer.eos_token_id,\n",
    "                                stopping_criteria=stopping_criteria,\n",
    "                                \n",
    "                                # use_cachе=True,\n",
    "                                # output_scores=True,\n",
    "                                # return_dict_in_generate=True).detach()\n",
    "        )\n",
    "        generated = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
    "        generated = remove_promt(prompt=prompt, generated=generated)\n",
    "        print(f\"tokens: {len(generated.split())}\")\n",
    "        print(f\"text:\\n{generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_armm_listing(model):\n",
    "\n",
    "    adapter_name = \"adapters/gpt-neox-20B-armm_list\"\n",
    "    model = PeftModel.from_pretrained(model, adapter_name, device_map={\"\":0})\n",
    "\n",
    "    generate(model, \"Q: Generate\\n A:\", max_length=512)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_armm_requirement(model):\n",
    "\n",
    "    adapter_name = \"adapters/gpt-neox-20B-armm_req\"\n",
    "    model = PeftModel.from_pretrained(model, adapter_name, device_map={\"\":0})\n",
    "\n",
    "    generate(model, \"Q: Generate\\n A:\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wm_resp_listing(model):\n",
    "\n",
    "    adapter_name = \"adapters/gpt-neox-20B-wm_list_resp\"\n",
    "    model = PeftModel.from_pretrained(model, adapter_name, device_map={\"\":0})\n",
    "\n",
    "    generate(model, \"Q: Generate response\\n A:\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wm_resp_purchase(model):\n",
    "\n",
    "    # adapter_name = \"adapters/gpt-neox-20B-wm_purch_resp\"\n",
    "    adapter_name = \"training/epoch-4.7\"\n",
    "    model = PeftModel.from_pretrained(model, adapter_name, device_map={\"\":0})\n",
    "\n",
    "    # generate(model, \"Q: Generate response\\n A:\", max_length=512)\n",
    "    generate(model, \"Q: Generate\\n A:\", max_length=512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate ARMM: listing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 76\n",
      "text:\n",
      " Hi {Client},\n",
      "\n",
      "Our client is seeking a {Model} {Category} {MakeModel}. We have a complete {SpareParts} available for the sale. The seller is asking {Price} {SpareParts}.\n",
      "\n",
      "They are hoping for the same day deal.\n",
      "\n",
      "We have a complete {SpareParts} available to complete the purchase at this time.\n",
      "\n",
      "They have released this unit to their supplier and they are hoping to move it soon.\n",
      "\n",
      "Let me know if you have any questions.\n",
      "\n",
      "Thank you.\n",
      "\n",
      "{EquipDetails}\n",
      "\n",
      "Best Regards,\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_armm_listing(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 83\n",
      "text:\n",
      " Hi {Client},\n",
      "\n",
      "We have a new {MakeModel} {Category} {Category} available for sale. It is currently being used by the user but they are looking to release it from the facility before the end of the year. \n",
      "\n",
      "{EquipDetails}\n",
      "\n",
      "This unit is in {MakeModel} {Category}. {Qty} units were made from the same tooling for {AipClient}. Please check their {SpareParts} for additional details. {Timeline}\n",
      "\n",
      "Please review this and let me know if you have any interest. Please let me know if you have additional requests. {Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_armm_listing(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 91\n",
      "text:\n",
      " Hi {Client},\n",
      "\n",
      "I hope you are well.  I have a client that is selling a {MakeModel}.  The seller wants to make a quick offer on this item before it goes for auction.  This item is only released to the market for {Qty} {Timeline}, and is still in the cleanroom in a clean and working condition.  The seller is only asking {Price} as a complete package.  Please see the photos below.  {EquipDetails}\n",
      "\n",
      "Please let me know if you have any questions and would like to have a discussion on this unit.\n",
      "\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_armm_listing(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 53\n",
      "text:\n",
      " Hi {Client},\n",
      "\n",
      "We have a used A4 {MakeModel} {Category} for sale at {Price} only, the seller said the seller didn t want to use the tool anymore. This {Category} has been used only for {Timeline} in complete working condition.\n",
      "\n",
      "Please review the photos and let me know if you are intersted.\n",
      "{EquipDetails}\n",
      "\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_armm_listing(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 64\n",
      "text:\n",
      " Hi {Client},\n",
      "\n",
      "I have some systems which I need to move. \n",
      "\n",
      "The system {MakeModel} was installed last year and the owner decided {Qty}, {Price}. It was a new system when it was installed, with only a few systems installed in the {EquipCategory}, so there are still a lot of options available for you. If you are interested, please let me know ASAP. \n",
      "\n",
      "{EquipConfig}\n",
      "\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_armm_listing(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate ARMM: requirement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 110\n",
      "text: \n",
      "\n",
      "Hi {Client},\n",
      "\n",
      "We have a need to purchase {MakeModel} for our client {SpareParts}, they need {WaferSize} {Category}. They will purchase it as well if they see it. \n",
      "\n",
      "Our team will be arriving {Timeline} for the purchase inspection. \n",
      "\n",
      "Our buyer would prefer to make a deal with the seller. \n",
      "\n",
      "If you have this tool available and you are looking to get rid of it, please kindly let me know your offer as soon as possible.\n",
      "\n",
      "I'm sorry that my client's budget is tight and we are looking for the tool in this week.  If you are interested in this, please kindly contact me.  My client's budget is {Timeline}.\n",
      "\n",
      "Thank you,\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_armm_requirement(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 62\n",
      "text:\n",
      " Hello {Client},\n",
      "\n",
      "We're seeking for a {Qty} {MakeModel} {Category} {WaferSize} {Timeline} for our client. This tool is required by the {MakeModel} to complete a project for a {MakeModel} client.\n",
      "\n",
      "Our client will also consider other options, like {MakeModel}, {Category}, {Category}.\n",
      "\n",
      "Our client would like to hear from you as soon as we can for a quick response.\n",
      "\n",
      "Thank you.\n",
      "\n",
      "Best regards,\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_armm_requirement(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 99\n",
      "text:\n",
      " Hi {Client},\n",
      "\n",
      "We have received a purchase request of {Qty} {MakeModel} system for a customer, who is planning to expand his existing facility to a new one. \n",
      "\n",
      "They would like to have a fully functional system as the existing system is still in working condition.\n",
      "\n",
      "Their preferred timeline to get their approval is {Timeline}. They are planning to make an offer within this time frame.\n",
      "\n",
      "I was wondering if you have any for sale, or if you have any other similar tools for sale. \n",
      "\n",
      "Please feel free to let me know, I can send you photos. \n",
      "\n",
      "Best regards, {Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_armm_requirement(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 91\n",
      "text:\n",
      " Hi  {Client},\n",
      "\n",
      "I have a {MakeModel} {Category} which is currently being used in my {SpareParts} business. I am in the process of reviewing our existing tools and would like to purchase this tool as well. It will be used in our {Timeline} {TimelineA} {TimelineB} for my client's toolset.\n",
      "\n",
      "It is preferred if you have the tool in hand. Please reply if you have one for sale. If you have one for sale, please also let me know your price range as well as any additional details regarding it.\n",
      "\n",
      "Best regards,\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_armm_requirement(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 51\n",
      "text:\n",
      " Dear {Client},\n",
      "I am working with a client looking to purchase a {MakeModel} for their facility. Do you have any of these units for sale at the moment? \n",
      "\n",
      "{WaferSize}\n",
      "\n",
      "If you do, please let me know so that I could schedule a site visit as soon as possible. \n",
      "\n",
      "Best regards,\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_armm_requirement(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate response for WM: New Listing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 43\n",
      "text:\n",
      " Hi {Client}, \n",
      " Thank you for reaching out. I'm {Trader} from CAE. Please send me the photos and configurations for the {Category} you want to sell. If you have any other {Category} you are interested in, please let me know.\n",
      " Thank you\n",
      " Best\n",
      "\n",
      "A: \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_wm_resp_listing(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 25\n",
      "text:\n",
      " Hi {Client}, \n",
      " \n",
      " Thanks for reaching out. Could you please send me some photos or a photo and configuration list for this unit? \n",
      " \n",
      " Best regards,\n",
      " {Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_wm_resp_listing(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 33\n",
      "text:\n",
      " Hi {Client}, \n",
      "\n",
      "I received your message about selling your {MakeModel}. \n",
      "  \n",
      "I am very interested in it. Do you have pictures of all the {MakeModel} you are willing to sell? \n",
      "\n",
      "Thanks. \n",
      "\n",
      "Best Regards,\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_wm_resp_listing(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 37\n",
      "text:\n",
      " Hi {Client},\n",
      "\n",
      "I received your request for a sale on these units. I've attached photos for you as well as an item listing. I'll be available after {Date} to take the sale off the market.\n",
      "\n",
      "Thanks,\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_wm_resp_listing(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 51\n",
      "text:\n",
      " Dear {Client}, \n",
      "  \n",
      "Good morning. This is {Trader} from CAE, I am contacting you on behalf of {Buyer}. We have your {MakeModel} {Category} {Category} system, we are interested in selling this machine. Could you send us photos of it and let me know your asking price?\n",
      "  \n",
      "Looking forward to your response,\n",
      "\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_wm_resp_listing(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate response for WM: Purchase Inquiry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 59\n",
      "text:\n",
      " Hello,\n",
      "\n",
      "I received your message about the webcam and I was able to locate the webcam. This is a photo of the unit and its configuration. The price includes the unit and the web setup fee. We do not have the unit in-house, but we can order it for you. Please let me know your timeline. \n",
      "\n",
      "{EquipDetails}\n",
      "\n",
      "{Signature}\n",
      "#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_wm_resp_purchase(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 23\n",
      "text:\n",
      " Hi {Client}, \n",
      "\n",
      "Good day. \n",
      "Please find my web inquiry below. Do you have any further information or requirement for this item? \n",
      "\n",
      "{EquipDetails}\n",
      "\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_wm_resp_purchase(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 94\n",
      "text:\n",
      " {Client}, \n",
      "\n",
      "{EquipDetails}\n",
      "\n",
      "Good morning. How are you, hope everything is good with you. This is {Trader} from CAE. I am contacting you in regards to one of our suppliers. We received an inquiry in our system and would like to know if you are still in the market of {MakeModel}, {Category}. \n",
      "Let me know what you are looking for and also if it's an offer or you would like to see it. If you do not currently own any of this model/category let me know and I will provide you with some information.\n",
      "\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_wm_resp_purchase(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 55\n",
      "text:\n",
      " Hello {Client}, \n",
      "\n",
      "Please note we received your request, \n",
      "\n",
      "This item is for sale. \n",
      "{EquipDetails} \n",
      "We have a limited number of this model. Please let me know the timeline that you are working with, and if you have other model you would want to trade in for a newer one, or purchase as-is. \n",
      "\n",
      "Thank you.\n",
      "\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_wm_resp_purchase(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 47\n",
      "text:\n",
      " {Client},\n",
      " This is {Trader} from CAE. I have received your inquiry about {MakeModel}\n",
      " Do you currently run this model? If you have any questions I'd be happy to help you. Please let me know the details below and we will follow up ASAP.\n",
      "\n",
      "{EquipDetails}\n",
      "\n",
      "Best Regards,\n",
      "{Signature}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_wm_resp_purchase(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
